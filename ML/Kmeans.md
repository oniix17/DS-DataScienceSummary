# K-means

### K-means算法步骤
* 确定聚类簇数（重心数量）：初始中心点随机选取
* 分配数据点：将数据点分配到最近的中心点
$$r_{ik}=
\begin{cases}
1& {k=argmin_l||x_i-z_l||^2}\\
0& {otherwise}
\end{cases}$$
* 更新中心点：迭代时选择同一类样本点的重心作为新的中心
$$Z_l=\frac{1}{N_l}\sum_{i=1}^Nr_{il}x_i(第l类的重心，r_{il}为数据点i是否属于l类，N_l是l类中点的个数)$$
* 循环2、3步骤，直到中心点和数据点都不再发生变化

### K-means中常用的到中心距离的度量
* 欧几里得距离（l2距离）
* 曼哈顿距离
$$L(A,B)=|x_a,x_b|+|y_a,y_b|+|z_a,z_b|+...$$

### K-means中的k值选取
* 手肘法：通过计算**误差平方和（Sum of the Square Error）**，当误差平方和较小时说明聚类效果较好。在画出SSE和k的图像后，找到斜率变化较大（陡变成平缓）的点，该点的k作为K-means的k。
$$SSE=\sum_{l=1}^k\sum_{p∈C_l}|p-z_l|^2$$
* 轮廓系数法

### K-means算法中初始点的选择对最终结果有影响吗？
有，不同初始点可能会导致不同的聚类结果。

### K-means聚类中每个类别中心的初始点如何选择？
* k个类的类别中心需要尽量远离
* 可以先使用*层次聚类*选择同簇的数据点，再通过计算得到类的中心点

### K-means中空聚类的处理
* 选择距离当前所有聚类中心最远的点
* 选择具有最大SSE的聚类，选择一个替补中心，将簇一分为二

### K-means是否会一直陷入选择质心的循环停不下来？
不会，可以证明Kmeans一定会收敛。而且直观来说当点的组合是有限的，所以一定会停止。加速停止的方法：
* 设置迭代次数
* 设置收敛判断距离

### K-means算法的优点和缺点是什么？
优点：
* 步骤清晰，可解释性强
* 算法简单

缺点：
* k较难确定
* 容易受异常值的影响
* *局部最优*（全局最优是NP-hard）
* 不同聚类中心的初始化可能对结果有较大影响
* 数据点只能是数值类型（数值型才能求中心）
* 对于非凸数据集或类别规模差异太大的数据效果不好
