# 决策树 (Desicion Tree)
* 已知特征的情况下，构建树形结构进行分析的模型（有监督分类/回归任务）

>* 内部节点：特征的测试
>* 分支：特征取值的可能结果
>* 叶结点：类别

* 主要模型：
>* ID3（基于**信息增益**）
>* C4.5（基于**信息增益比**）
>* CART（基于**二叉决策树**）
>* RF（基于**并行集成**决策树）
>* GBDT/XGBoost/LightGBM（基于**串行集成**决策树）

* 信息熵（entropy）：描述信息**不确定性**的值（也为**信息量**的值，该值越小纯度越高）
> 假设数据集D有x类，其中第k类占比为$p_k$，则信息熵公式为
>$$entropy(D)=-\sum_{i=1}^{x}p_i log_2(p_i)$$
> 信息熵的最小值为0（不确定度最小，纯度最高，概率要么为1要么为0），信息熵最大为$log_2|x|$（均匀分布）
* 信息增益（Information Gain）：选择某个特征属性进行划分时信息熵的变化（描述了该特征带来的信息量的多少）
> $$Gain(D,A)=entropy(D)-entropy(D|A)$$
> $$=-entropy(D)-\sum_{i=1}^{v}\frac{|D_v|}{|D|} entropy(D_v)$$
> 其中entropy(D|A)为特征A给定条件下D的经验**条件熵**（A的取值为{ $a_1,a_2,...,a_v$ }, $D_v$ 为D中A属性为 $a_v$ 的集合）



这里以西瓜数据集为例，瓜分为好瓜、坏瓜，所以$|x|=2$。其中总瓜数为17，好瓜为8，坏瓜为9。
根结点包含 公式 个训练样例，其中好瓜共计 公式 个样例，所占比例为 公式。
坏瓜共计 公式 个样例，所占比例为 公式。
将数据带入信息熵公式，即可得到根结点的信息熵。

决策树算法详解; 最优属性选择; 信息增益示例;



以属性「色泽」为例，其对应的 公式 个数据子集：

公式，包含 公式，公式 个样例，其中好瓜样例为 公式，比例为 公式，坏瓜样例为 公式，比例为 公式。将数据带入信息熵计算公式即可得到该结点的信息熵。

公式，包含 公式，公式 个样例，其中好瓜样例为 公式，比例为 公式，坏瓜样例为 公式，比例为 公式。将数据带入信息熵计算公式即可得到该结点的信息熵。

公式，包含 公式，公式 个样例，其中好瓜样例为 公式，比例为 公式，坏瓜样例为 公式，比例为 公式。将数据带入信息熵计算公式即可得到该结点的信息熵。

决策树算法详解; 最优属性选择; 信息增益示例;



色泽属性的信息增益为：

决策树算法详解; 最优属性选择; 信息增益示例;



# GBDT (Gradient Boosting Decision Tree)
* GBDT中的决策树为回归树
* 
