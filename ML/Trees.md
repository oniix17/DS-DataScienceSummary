# 决策树 (Desicion Tree)
* 已知特征的情况下，构建树形结构进行分析的模型（有监督分类/回归任务）

>* 内部节点：特征的测试
>* 分支：特征取值的可能结果
>* 叶结点：类别

* 主要模型：
>* ID3（基于**信息增益**）
>* C4.5（基于**信息增益比**）
>* CART（基于**二叉决策树**）
>* RF（基于**并行集成**决策树）
>* GBDT/XGBoost/LightGBM（基于**串行集成**决策树）

### 信息熵（entropy）
* 描述信息**不确定性**的值（也为**信息量**的值，该值越小纯度越高）
> 假设数据集D有x类，其中第k类占比为$p_k$，则信息熵公式为
> $$entropy(D)=-\sum_{i=1}^{x}p_i log_2(p_i)$$
> 信息熵的最小值为0（不确定度最小，纯度最高，概率要么为1要么为0），信息熵最大为$log_2|x|$（均匀分布）

### 信息增益（Information Gain）
* 选择某个特征属性进行划分时信息熵的变化（描述了该特征带来的信息量的多少）
* *ID3基于该最优属性选择方法*，每步选择信息增益最大的属性作为分裂节点
> $$Gain(D,A)=entropy(D)-entropy(D|A)$$
> $$=-entropy(D)-\sum_{i=1}^{v}\frac{|D_v|}{|D|} entropy(D_v)$$
> 其中entropy(D|A)为特征A给定条件下D的经验**条件熵**（A的取值为{ $a_1,a_2,...,a_v$ }, $D_v$ 为D中A属性为 $a_v$ 的集合）
> （注意这里x是类别的数量，而v是进行划分时特征的数量）

这里以西瓜数据集为例[1]，瓜分为好瓜、坏瓜，所以$x=2$。其中总瓜数为17，好瓜为8，坏瓜为9。
> 信息熵$entropy(D)=-(\frac{8}{17}\log_2(\frac{8}{17})+\frac{9}{17}\log_2(\frac{9}{17}))$
> 如果使用ID3（基于信息增益），假设所选的特征为色泽：
>* D1（色泽=青绿）的总样本数为6，其中好瓜为3，坏瓜为3，信息熵为$entropy(D_1)=-(\frac{3}{6}log_2(\frac{3}{6})+\frac{3}{6}log_2(\frac{3}{6}))$ <br>
>* D2（色泽=乌黑）的总样本数为6，其中好瓜为4，坏瓜为2，信息熵为$entropy(D_2)=-(\frac{4}{6}log_2(\frac{4}{6})+\frac{2}{6}log_2(\frac{2}{6}))$ <br>
>* D3（色泽=浅白）的总样本数为5，其中好瓜为1，坏瓜为4，信息熵为$entropy(D_2)=-(\frac{1}{6}log_2(\frac{1}{6})+\frac{4}{6}log_2(\frac{4}{6}))$ 

信息增益为:
> $$Gain(D,A)=entropy(D)-\sum_{i=1}^{3}\frac{|D_i|}{D}entropy(D_i)$$
> $$=-(\frac{8}{17}\log_2(\frac{8}{17})+\frac{9}{17}\log_2(\frac{9}{17}))-\frac{6}{17}(-(\frac{1}{2}log_2(\frac{1}{2})+\frac{1}{2}log_2(\frac{1}{2})))-\frac{6}{17}(-(\frac{2}{3}log_2(\frac{2}{3})+\frac{1}{3}log_2(\frac{1}{3})))-\frac{5}{17}(-(\frac{1}{5}log_2(\frac{1}{5})+\frac{4}{5}log_2(\frac{4}{5})))$$

### 信息增益率（Gain Ratio）
*C4.5基于这种最优属性选择方法*，每步选择信息增益率最大的属性作为分裂节点
>* 信息增益较容易受到特征取值的数量的影响（通常来说，特征取值数量越多（上面的x越大），越倾向于产生熵较小的划分结果，此时信息增益较大，故信息增益倾向于取值较多的特征），而信息增益率通过除以固有值(intrinsic value)减小这种倾向

信息增益率公式：
$$GainRatio(D,A)=\frac{Gain(D,A)}{IV(A)}$$
$$IV(A)=-\sum_{i=1}^{v}\frac{|D_v|}{|D|}log_2\frac{|D_v|}{|D|}$$
>* 其中IV用于衡量属性的分散程度

### Gini指数（Gini Index）
*CART基于这种最优属性选择方法*（CART与ID3,C4.5不同：CART是二叉树），每步要求基尼系数最小
#### CART（classification and regression tree）
* 二叉树（特征取值为“是”或“否”）
>* 类似于信息熵，Gini指数的值越小，则数据集纯度越高
> $Gini(D)=-\sum_{i=1}^{x}\sum_{j≠i}p_ip_j=1-\sum_{i=1}^{x}p_i^2$
>* $p_i$为第i类占比

### 过拟合
树过深、树枝过多可能导致过拟合（过拟合：*过于紧密或精确地匹配特定数据集，以致于无法良好地拟合其他数据的现象*），利用*剪枝*降低过拟合风险
> 预剪枝（pre-pruning）：在树模型训练前进行，如果结点的划分不能带来决策树*泛化性能*的提升，则停止划分并将该结点标记为叶结点（通过预剪枝最终得到仅有一层划分的决策树，称为*决策树桩*（decision stump））  
> 后剪枝（post-pruning）在树模型训练之后**自底向上**进行，如果将*子树*替换为叶结点后决策树*泛化性能*提升，则将该子树替换为叶结点
* 预剪枝训练时间降低，测试时间降低；后剪枝训练时间增加，测试时间降低
* 预剪枝过拟合风险减小，欠拟合风险增加；后剪枝过拟合风险减小，欠拟合风险几乎不变（后剪枝的泛化性能优于预剪枝）

### 连续值/离散值处理
* 离散值：选择一个特征，根据特征的不同值进行划分
* 连续值：需要进行离散化，策略主要采用**二分法**（这也是*C4.5*中采用的策略）

<br>
<br>
<br>

# 梯度提升决策树 GBDT (Gradient Boosting Decision Tree) 
又称为MART(Multiple Additive Regression Tree) ，由多棵决策树*串行训练*生成（boosting）。基分类器层层叠加，对于上一层错误分类的样本给予更高的权重（boosting基学习器之间存在强依赖，而bagging基学习器之间基本不存在强依赖）；预测时将样本直接相加或加权得到最终结果。

### GBDT原理：
> 弱分类器结果相加为最终结果。
> 以当前预测为基准，下一*弱分类器*对于残差（预测值和真实值之间的误差）进行拟合。
> GBDT中弱分类器为树模型(具体而言决策树为回归树)。
在回归任务中，GBDT通过计算损失函数的负梯度更新模型（在损失函数为平方误差时，负梯度为残差）。每一轮迭代中当前树都在学习之前树的不足。
$$F_{i+1}=F_i-η▽_FL|F=F_{i}$$
$$L=\sum_{i=1}^nl(y_i,F(x_i))$$
梯度提升（GB）与梯度下降（GD）的不同之处在于：梯度下降是对于参数ω进行求导，而梯度提升是直接对于函数F（理解为$\hat{y_i}$）进行求导。

### GDBT缺点：
* 适合于数值类型数据，离散类型效果不佳（可能是因为需要进行梯度计算？）
* 训练时不能并行生成树模型（后一棵树的训练以及结果是基于前一棵树的）
* 容易受到异常值的影响（应该算是boosting的通病？）


### 与随机森林的异同点
相同点：
* 都是通过决策树集成进行构建 

不同点：
* 随机森林使用bagging构建树模型，而GBDT使用boosting构建树模型（随机森林可以并行生成而GBDT只能串行生成）
* 随机森林对于异常值不如GBDT敏感
* 随机森林降低方差而GBDT降低偏差（也跟bagging和boosting的构成相关）

# XGBoost
### 与GBDT的异同点：
相同点：
* 二者都是加法模型，都在每一步中都只更新当前步的子模型。 

不同点：
* XGBoost采用了列采样（每一次划分时都只考虑部分特征）而GBDT没有。
* XGBoost采用了正则项（对于当前步的子树进行，例如子树的叶结点个数，叶结点的回归值构成的向量（L2 norm））而GBDT没有。
* XGBoost考虑了缺失值或稀疏0值的处理：稀疏感知（将稀疏0值和缺失值都视为缺失，节点分裂时固定这些值，而只考虑非缺失值的切分）。
* XGBoost基于一阶导和二阶导，而GBDT只基于一阶导。

![image](https://github.com/oniix17/InterviewPreparation/blob/main/images/XGBoost1.jpg)



[1]https://www.showmeai.tech/article-detail/190
